{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All you need is love… And a pet!\n",
    "\n",
    "<img src=\"img/dataset-cover.jpg\" width=\"920\">\n",
    "\n",
    "Here we are going to build a classifier to predict whether an animal from an animal shelter will be adopted or not (aac_intakes_outcomes.csv, available at: https://www.kaggle.com/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes/version/1#aac_intakes_outcomes.csv). You will be working with the following features:\n",
    "\n",
    "1. *animal_type:* Type of animal. May be one of 'cat', 'dog', 'bird', etc.\n",
    "2. *intake_year:* Year of intake\n",
    "3. *intake_condition:* The intake condition of the animal. Can be one of 'normal', 'injured', 'sick', etc.\n",
    "4. *intake_number:* The intake number denoting the number of occurrences the animal has been brought into the shelter. Values higher than 1 indicate the animal has been taken into the shelter on more than one occasion.\n",
    "5. *intake_type:* The type of intake, for example, 'stray', 'owner surrender', etc.\n",
    "6. *sex_upon_intake:* The gender of the animal and if it has been spayed or neutered at the time of intake\n",
    "7. *age_upon\\_intake_(years):* The age of the animal upon intake represented in years\n",
    "8. *time_in_shelter_days:* Numeric value denoting the number of days the animal remained at the shelter from intake to outcome.\n",
    "9. *sex_upon_outcome:* The gender of the animal and if it has been spayed or neutered at time of outcome\n",
    "10. *age_upon\\_outcome_(years):* The age of the animal upon outcome represented in years\n",
    "11. *outcome_type:* The outcome type. Can be one of ‘adopted’, ‘transferred’, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from itertools import combinations \n",
    "import ast\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Load the dataset and convert categorical features to a suitable numerical representation (use dummy-variable encoding). \n",
    "- Split the data into a training set (80%) and a test set (20%). Pair each feature vector with the corresponding label, i.e., whether the outcome_type is adoption or not. \n",
    "- Standardize the values of each feature in the data to have mean 0 and variance 1.\n",
    "\n",
    "The use of external libraries is not permitted in part A, except for numpy and pandas. \n",
    "You can drop entries with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['animal_type', 'intake_year', 'intake_condition', 'intake_number', 'intake_type', 'sex_upon_intake', \\\n",
    "          'age_upon_intake_(years)', 'time_in_shelter_days', 'sex_upon_outcome', 'age_upon_outcome_(years)', \\\n",
    "          'outcome_type']\n",
    "original_data = pd.read_csv(data_folder+'aac_intakes_outcomes.csv', usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The length of the data with all rows is : {}'.format(len(original_data)))\n",
    "original_data.dropna(inplace=True)\n",
    "print('The length of the data without the rows with nan value is: {}'.format(len(original_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = original_data.copy()\n",
    "data_features['adopted'] = data_features.outcome_type.apply(lambda r: 1 if r=='Adoption' else 0)\n",
    "data_features.drop(\"outcome_type\", axis = 1, inplace=True)\n",
    "data_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set(data_to_split, ratio=0.8):\n",
    "    mask = np.random.rand(len(data_to_split)) < ratio\n",
    "    return [data_to_split[mask].reset_index(drop=True), data_to_split[~mask].reset_index(drop=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train, test] = split_set(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex_upon_outcome', 'animal_type', 'intake_condition',\n",
    "                       'intake_type', 'sex_upon_intake']\n",
    "train_categorical = pd.get_dummies(train, columns=categorical_columns)\n",
    "train_categorical.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in the testing set must be matched with the traning set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we use only the features available in the training set\n",
    "test_categorical = pd.get_dummies(test, columns=categorical_columns)[train_categorical.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_categorical.adopted\n",
    "train_features = train_categorical.drop('adopted', axis=1)\n",
    "print('Length of the train dataset : {}'.format(len(train)))\n",
    "\n",
    "test_label=test_categorical.adopted\n",
    "test_features = test_categorical.drop('adopted', axis=1)\n",
    "print('Length of the test dataset : {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = train_features.mean()\n",
    "stddevs = train_features.std()\n",
    "\n",
    "train_features_std = pd.DataFrame()\n",
    "for c in train_features.columns:\n",
    "    train_features_std[c] = (train_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "# Use the mean and stddev of the training set\n",
    "test_features_std = pd.DataFrame()\n",
    "for c in test_features.columns:\n",
    "    test_features_std[c] = (test_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "train_features_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Train a logistic regression classifier on your training set. Logistic regression returns probabilities as predictions, so in order to arrive at a binary prediction, you need to put a threshold on the predicted probabilities. \n",
    "- For the decision threshold of 0.5, present the performance of your classifier on the test set by displaying the confusion matrix. Based on the confusion matrix, manually calculate accuracy, precision, recall, and F1-score with respect to the positive and the negative class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(true_label, prediction_proba, decision_threshold=0.5): \n",
    "    \n",
    "    predict_label = (prediction_proba[:,1]>decision_threshold).astype(int)   \n",
    "                                                                                                                       \n",
    "    TP = np.sum(np.logical_and(predict_label==1, true_label==1))\n",
    "    TN = np.sum(np.logical_and(predict_label==0, true_label==0))\n",
    "    FP = np.sum(np.logical_and(predict_label==1, true_label==0))\n",
    "    FN = np.sum(np.logical_and(predict_label==0, true_label==1))\n",
    "    \n",
    "    confusion_matrix = np.asarray([[TP, FP],\n",
    "                                    [FN, TN]])\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix\n",
    "    label = np.asarray([['TP {}'.format(TP), 'FP {}'.format(FP)],\n",
    "                        ['FN {}'.format(FN), 'TN {}'.format(TN)]])\n",
    "    \n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=['Yes', 'No'], columns=['Positive', 'Negative']) \n",
    "    \n",
    "    return sn.heatmap(df_cm, cmap='YlOrRd', annot=label, annot_kws={\"size\": 16}, cbar=False, fmt='')\n",
    "\n",
    "\n",
    "def compute_all_score(confusion_matrix, t=0.5):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix.astype(float)\n",
    "    \n",
    "    accuracy =  (TP+TN)/np.sum(confusion_matrix)\n",
    "    \n",
    "    precision_positive = TP/(TP+FP) if (TP+FP) !=0 else np.nan\n",
    "    precision_negative = TN/(TN+FN) if (TN+FN) !=0 else np.nan\n",
    "    \n",
    "    recall_positive = TP/(TP+FN) if (TP+FN) !=0 else np.nan\n",
    "    recall_negative = TN/(TN+FP) if (TN+FP) !=0 else np.nan\n",
    "\n",
    "    F1_score_positive = 2 *(precision_positive*recall_positive)/(precision_positive+recall_positive) if (precision_positive+recall_positive) !=0 else np.nan\n",
    "    F1_score_negative = 2 *(precision_negative*recall_negative)/(precision_negative+recall_negative) if (precision_negative+recall_negative) !=0 else np.nan\n",
    "\n",
    "    return [t, accuracy, precision_positive, recall_positive, F1_score_positive, precision_negative, recall_negative, F1_score_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logistic.fit(train_features_std,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = logistic.predict_proba(test_features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_05 = compute_confusion_matrix(test_label, prediction_proba, 0.5 )\n",
    "plt.figure(figsize = (4,3)) \n",
    "ax = plot_confusion_matrix(confusion_matrix_05)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Confusion matrix for a 0.5 threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t, accuracy, precision_positive, recall_positive, F1_score_positive, \\\n",
    "    precision_negative, recall_negative, F1_score_negative] = compute_all_score(confusion_matrix_05)\n",
    "\n",
    "print(\"The accuracy of this model is {0:1.3f}\".format(accuracy))\n",
    "print(\"For the positive case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_positive, recall_positive, F1_score_positive))\n",
    "print(\"For the negative case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_negative, recall_negative, F1_score_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Vary the value of the threshold in the range from 0 to 1 and visualize the value of accuracy, precision, recall, and F1-score (with respect to both classes) as a function of the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_score_name = ['Threshold', 'Accuracy', 'Precision P', 'Recall P', 'F1 score P', \\\n",
    "                                              'Precision N', 'Recall N', 'F1 score N']\n",
    "threshold_score = pd.concat([pd.DataFrame([compute_all_score(compute_confusion_matrix(test_label, prediction_proba, t ),t)]\\\n",
    "                                             , columns=columns_score_name) for t in threshold], ignore_index=True)\n",
    "threshold_score.set_index('Threshold', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_score['Accuracy'].plot(grid=True).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(10,5))\n",
    "\n",
    "col_plot = ['Precision P', 'Recall P', 'F1 score P', 'Precision N', 'Recall N', 'F1 score N']\n",
    "\n",
    "major_ticks = np.linspace(0,1,5)\n",
    "\n",
    "for axe, col in zip(axs.flat, col_plot):\n",
    "    threshold_score[col].plot(ax=axe, grid = True)\n",
    "    axe.set_title(col)\n",
    "    axe.set_xticks(major_ticks)    \n",
    "    axe.grid(which='major', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Plot in a bar chart the coefficients of the logistic regression sorted by their contribution to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logistic.fit(train_features_std, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for name, value in zip(train_features_std.columns, logistic.coef_[0]):\n",
    "    tmp.append({\"name\": name, \"value\": value})\n",
    "    \n",
    "features_coef = pd.DataFrame(tmp).sort_values(\"value\")\n",
    "features_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(5,7))\n",
    "plt.barh(features_coef.name, features_coef.value, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "## Questions 1: Which of the following metrics is most suitable when you are dealing with unbalanced classes?\n",
    "\n",
    "- **a) F1 Score**\n",
    "- b) Recall\n",
    "- c) Precision\n",
    "- d) Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: You are working on a binary classification problem. You trained a model on a training dataset and got the following confusion matrix on the test dataset. What is true about the evaluation metrics (rounded to the second decimal point):\n",
    "\n",
    "|            | Pred = NO|Pred=YES|\n",
    "|------------|----------|--------|\n",
    "| Actual NO  |    50    |   10   |\n",
    "| Actual YES |    5     |   100  |\n",
    "\n",
    "- a) Accuracy is 0.95\n",
    "- b) Accuracy is 0.85\n",
    "- c) False positive rate is 0.95\n",
    "- **d) True positive rate is 0.95**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
